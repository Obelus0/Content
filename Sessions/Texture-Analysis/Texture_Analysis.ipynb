{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![cvi_logo.png](./nb_images/cvi_logo.png)\n",
    "#                        Computer Vision and Intelligence Group, CFI- IIT Madras\n",
    "\n",
    "**Topics going to be covered:**\n",
    "    \n",
    "  **Geometrical Transformations,Thresholding,K-means Clustering,Contour Analysis**  \n",
    "\n",
    "**Download this notebook from our Github repository: https://github.com/iitmcvg/Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Transformations of Images\n",
    "\n",
    "# Goals\n",
    "\n",
    "**Learn to apply different geometric transformation to images like Scaling,Translation, Rotation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling\n",
    "\n",
    "**Scaling is just resizing of the image. OpenCV comes with a function cv2.resize() for this purpose.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_imgs/messi.png')\n",
    "\n",
    "res = cv2.resize(img,None,fx=1/2, fy=1/2, interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "cv2.imshow('img1',img)\n",
    "cv2.imshow('img2',res)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation\n",
    "**Translation is the shifting of object’s location. If you know the shift in (x,y) direction, let it be , you can create the transformation matrix  as follows:**\n",
    "\n",
    "![translation.png](./nb_images/translation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_imgs/messi.png',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = np.float32([[1,0,200],[0,1,100]])\n",
    "translated = cv2.warpAffine(img,M,(cols,rows))\n",
    "\n",
    "cv2.imshow('img1',img)\n",
    "cv2.imshow('img2',translated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotation\n",
    "**Rotation of an image for an angle  is achieved by the transformation matrix of the form**\n",
    "![rotation1.png](./nb_images/rotation1.png)\n",
    "\n",
    "But OpenCV provides scaled rotation with adjustable center of rotation so that you can rotate at any location you prefer. Modified transformation matrix is given by\n",
    "![rotation2.png](./nb_images/rotation2.png)\n",
    "\n",
    "**where:**\n",
    "![rotation3.png](./nb_images/rotation3.png)\n",
    "\n",
    "To find this transformation matrix, OpenCV provides a function, cv2.getRotationMatrix2D\n",
    "\n",
    "**cv2.getRotationMatrix2D(center, angle, scale) returns a 2x3 matrix for warp affine,where:**\n",
    "\n",
    "**center** – Center of the rotation in the source image.\n",
    "\n",
    "**angle** – Rotation angle in degrees. Positive values mean counter-clockwise rotation (the coordinate origin is assumed to be the top-left corner)\n",
    "\n",
    "**scale** – Scaling factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_imgs/ronaldo.png',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "M = cv2.getRotationMatrix2D((cols/2,rows/2),180,1)\n",
    "dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "cv2.imshow('img1',img)\n",
    "cv2.imshow('img2',dst)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Thresholding\n",
    "\n",
    "## Thresholding\n",
    "\n",
    "Thresholding is the most simple segmentation methods. It basically is used to segment the image of interest based the pixel values. \n",
    "\n",
    "\n",
    "### Simple Thresholding\n",
    "\n",
    "This is extremely simple. If the pixel value is greater than your threshold, it is assigned a particular value, or else, it is assigned another value. \n",
    "\n",
    "#### There are the following types of Simple Thresholding:\n",
    "\n",
    "* cv2.THRESH_BINARY\n",
    "* cv2.THRESH_BINARY_INV\n",
    "* cv2.THRESH_TRUNC\n",
    "* cv2.THRESH_TOZERO\n",
    "* cv2.THRESH_TOZERO_INV\n",
    "\n",
    "They act in the following way:\n",
    "    \n",
    "![gradient_outputs.jpg](./nb_images/gradient_outputs.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample code for Simple Thresholding\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread('./test_imgs/Cvi3.jpg', 0)\n",
    "ret, thresh1= cv2.threshold(img,127,255,cv2.THRESH_TOZERO)\n",
    "cv2.imshow('thresh1',thresh1)\n",
    "if cv2.waitKey(0) & 0xff == 27:  \n",
    "    cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Simple Thresholding fails and the need for Adaptive Thresholding\n",
    "\n",
    "As you guys saw, we set one global value as the threshold. There might be cases where the image is taken in different lighting conditions and hence the threshold value should ideally change. \n",
    "\n",
    "Moreover, there might be differential lighting within the image itself. In such cases, Simple Thresholding fails miserably.\n",
    " \n",
    "![CVi3.jpg](./nb_images/CVi3.jpg)\n",
    "\n",
    "Simple Thresholding on this image gives you:\n",
    "\n",
    "![CVi3_simple.jpg](./nb_images/CVi3_simple.jpg)\n",
    "\n",
    "This is where Adaptive Thresholding comes in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Adaptive Thresholding\n",
    "\n",
    "Adaptive Thresholding uses an algorithm that determines the threshold for a pixel based on a small region around it. As a result, we get different thresholds for different regions of the same image.It gives better results for images, especially those which have varying illumination.\n",
    "\n",
    "#### Types of Adaptive Thresholding\n",
    "\n",
    "*  Adaptive Mean Thresholding\n",
    "*  Adaptive Gaussian Thresholding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Syntax for Adaptive Thresholding\n",
    "cv.AdaptiveThreshold(src, maxValue, adaptive_method, thresholdType, blockSize, c)\n",
    "\n",
    "#src = Source Image\n",
    "#maxValue - Non-zero value assigned to the pixels for which the condition is satisfied.\n",
    "#adaptiveMethod - Adaptive thresholding algorithm to use, ADAPTIVE_THRESH_MEAN_C or ADAPTIVE_THRESH_GAUSSIAN_C \n",
    "#thresholdType - Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV\n",
    "#blockSize - size of a pixel neighborhood that is used to calculate a threshold value for the pixel: 3, 5, 7, and so on.\n",
    "#C - Constant subtracted from the mean or weighted mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Code for Adaptive Thresholding\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread('./test_imgs/CVI3.jpg', 0)\n",
    "th2 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_MEAN_C,cv2.THRESH_BINARY,11, 6)\n",
    "th3 = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY,11, 6)\n",
    "cv2.imshow('Adaptive Mean',th2)\n",
    "cv2.imshow('Adaptive Gaussian',th3)\n",
    "if cv2.waitKey(0) & 0xff == 27:  \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Mean Thresholding \n",
    "\n",
    "Here, the threshold value is calculated as the mean of the neighbourhood area minus the constant C.\n",
    "\n",
    "*cv2.ADAPTIVE_THRESH_MEAN_C*\n",
    "\n",
    "![Cvi3_mean.jpg](./nb_images/Cvi3_mean.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Gaussian Thresholding \n",
    "\n",
    "Here, the threshold value is calculated as the gaussian-weighted sum of the neighbourhood values minus the constant C.\n",
    "\n",
    "*cv2.ADAPTIVE_THRESH_GAUSSIAN_C*\n",
    "\n",
    "![CVi3_gaussian.jpg](./nb_images/CVi3_gaussian.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                    K-Means Clustering\n",
    "K-Means is a type of clustering that involves classifying a set of data into \"K\" groups on the basis of their proximities to K-Means/K-centroids.\n",
    "The process of classification and centroid adjustment is repeated until the values of the centroids stabilize\n",
    "The final centroids will be used to produce the final classification/clustering of the input data, effectively turning the set of initially anonymous data points into a set of data points, each with a class identity. \n",
    "\n",
    "Let **k = {2,3,4,10,11,12,20,25,30}**\n",
    "\n",
    "Let **numbers of clusters to be made = 2 and m1 = 4 , m2 = 12**\n",
    "\n",
    "Based on proximity of elements of k to m1 and m2\n",
    "\n",
    "**STEP 1:**\n",
    "\n",
    "**k1 = {2,3,4}        and     k2 = {10,11,12,20,25,30}**\n",
    "\n",
    "**m1 = (2+3+4)/3 = 3  and     m2 = (10+11+12+20+25+30)/6 = 18**\n",
    "\n",
    "**STEP 2:**\n",
    "\n",
    "**k1 = {2,3,4,10}        and     k2 = {11,12,20,25,30}**\n",
    "\n",
    "**m1 = 4.75  and     m2 = 19.6**\n",
    "\n",
    "**STEP 3:**\n",
    "\n",
    "**k1 = {2,3,4,10,11,12}        and     k2 = {20,25,30}**\n",
    "\n",
    "**m1 = 7  and     m2 = 19.6**\n",
    "\n",
    "**STEP 4:**\n",
    "\n",
    "**k1 = {2,3,4,10,11,12}        and     k2 = {20,25,30}**\n",
    "\n",
    "**m1 = 7  and     m2 = 19.6**\n",
    "\n",
    "STEP 3 and STEP 4 have same clusters which means clustering is terminated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Let's say that a company that a company designs t-shirts based on height and weight of people.**\n",
    "\n",
    "![km1.png](./nb_images/km1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X = np.random.randint(25,50,(25,2)) #random 25 pairs of integers b/w 25 and 50\n",
    "Y = np.random.randint(60,85,(25,2)) #random 25 pairs of integers b/w 60 and 85\n",
    "Z = np.vstack((X,Y))  #converting two (25x2)vectors into (50x2)vector\n",
    "\n",
    "# convert to np.float32\n",
    "Z = np.float32(Z)\n",
    "\n",
    "# define criteria and apply kmeans()\n",
    "k=2\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "ret,label,center=cv2.kmeans(Z,k,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Now separate the data, Note the flatten()\n",
    "A = Z[label.ravel()==0]\n",
    "B = Z[label.ravel()==1]\n",
    "\n",
    "# Plot the data\n",
    "plt.scatter(A[:,0],A[:,1])\n",
    "plt.scatter(B[:,0],B[:,1],c = 'r')\n",
    "plt.scatter(center[:,0],center[:,1],s = 80,c = 'y', marker = 's')\n",
    "plt.xlabel('Height'),plt.ylabel('Weight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./test_imgs/lenna.png')\n",
    "Z = img.reshape((-1,3))\n",
    "\n",
    "# convert to np.float32\n",
    "Z = np.float32(Z)\n",
    "\n",
    "# define criteria, number of clusters(K) and apply kmeans()\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "K = 3\n",
    "ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Now convert back into uint8, and make original image\n",
    "center = np.uint8(center)\n",
    "res = center[label.flatten()]\n",
    "res2 = res.reshape((img.shape))\n",
    "\n",
    "#cv2.namedWindow('res2',cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('res1',img)\n",
    "cv2.imshow('res2',res2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contouring\n",
    "\n",
    "Contours can be explained simply as a curve joining all the continuous points (along the boundary), having same color or intensity. The contours are a useful tool for shape analysis and object detection and recognition.\n",
    "\n",
    "* For better accuracy, use binary images. So before finding contours, apply threshold or canny edge detection.\n",
    "* In OpenCV, finding contours is like finding white object from black background. So remember, object to be found should be white and background should be black.\n",
    "\n",
    "### cv.findContours()\n",
    "\n",
    "This is the basic function used to find the coordinates of the contours in the image.\n",
    "\n",
    "**Syntax:** contours, hierarchy = cv2.findContours(image, mode, method)\n",
    "\n",
    "* image: The source image, whose contours you would like to find.\n",
    "* mode: The contour retrieval mode. Some of them are:\n",
    " - cv.RETR_EXTERNAL\n",
    " - cv.RETR_TREE \n",
    " - cv.RETR_LIST\n",
    " - cv.RETR_CCOMP\n",
    "* method: \n",
    " - cv.CHAIN_APPROX_NONE\n",
    " - cv.CHAIN_APPROX_SIMPLE\n",
    " - cv.CHAIN_APPROX_TC89_L1 etc\n",
    "\n",
    "cv.findContours() returns 2 things:\n",
    "\n",
    "* contours: Python list of all the contours in the image. Each individual contour is a Numpy array of (x,y) coordinates of boundary points of the object.\n",
    "* hierarchy: The final return value is a NumPy array that contains hierarchy information about the contours.\n",
    "\n",
    "\n",
    "![cvi4.jpg](./test_imgs/cvi4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img = cv2.imread('./test_imgs/cvi4.jpg', 0)\n",
    "ret, thresh = cv2.threshold(img,220,255,cv2.THRESH_BINARY_INV)\n",
    "_,contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "print(contours, hierarchy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cv.drawContours\n",
    "\n",
    "This is the basic function used to draw the contours you have found for the concerned image. It can also be used to draw any shape provided you have its boundary points. \n",
    "\n",
    "**Syntax:** image\t=\tcv.drawContours(image, contours, contouridx, color, thickness)\n",
    "\n",
    "* image: The source image, on which you would like to draw the contours.\n",
    "* contours: The contours which should be passed as a Python list. It should contain all the contours as a list.\n",
    "* contouridx: Parameter indicating a contour to draw. If it is negative, all the contours are drawn.\n",
    "* color: Color of the contours. \n",
    "* thickness: Thickness of lines the contours are drawn with. If it is negative (for example, thickness=FILLED ), the contour interiors are drawn.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Moments\n",
    "\n",
    "Image moments help you to calculate some features like center of mass of the object, area of the object etc.\n",
    "An inbuilt function **cv2.moments()** automatically calculates all the moments of the concerned area.\n",
    "It returns a dictionary containing all the moments.\n",
    "\n",
    "**Syntax**: retval\t=\tcv.moments(array)\n",
    "\n",
    "* dict: The dictionary of all the moments.\n",
    "* array: An array of 2-D points which form the object.\n",
    "\n",
    "#### Finding the Centroid\n",
    "\n",
    "Let cx, cy be the coordinates of the Centroid:\n",
    "\n",
    "cx = int(m10/m00)\n",
    "cy = int(m01/m00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = cv2.imread('./test_imgs/cvi4.jpg', 0)\n",
    "img1 = cv2.imread('./test_imgs/cvi4.jpg')\n",
    "ret, thresh = cv2.threshold(img,220,255,cv2.THRESH_BINARY_INV)\n",
    "_,contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnt = contours[2]\n",
    "M = cv2.moments(cnt)\n",
    "print(M)\n",
    "cx = int(M['m10']/M['m00'])\n",
    "cy = int(M['m01']/M['m00'])\n",
    "img1 = cv2.circle(img1, ( cx, cy), 10, (0,255,0), 3) \n",
    "cv2.imshow('contours', img1)\n",
    "if cv2.waitKey(0) & 0xff == 27:  \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Area\n",
    "\n",
    "The Area can be trivially found out if you know the moments of the concerned contour.\n",
    "It is just equal to **m00** moment.\n",
    "\n",
    "OpenCV also has a built-in function to calculate the area : cv.contourArea()\n",
    "It has only one argument: The contour for which you would want to find the area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area = cv2.contourArea(cnt)\n",
    "print(area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the perimeter\n",
    "\n",
    "Again, OpenCV has a built in function which returns the contour perimeter or a curve length.\n",
    "\n",
    "**Syntax** : perimeter\t= cv.arcLength(curve, closed)\n",
    "\n",
    "* curve: Input vector of 2D points or the contour\n",
    "* closed: A flag indicating whether the curve is closed or not. It's True for a contour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perimeter = cv2.arcLength(cnt,True)\n",
    "print(perimeter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Rectangles\n",
    "\n",
    "#### Straight  Bounding Rectangles\n",
    "\n",
    "It is a straight rectangle, it doesn't consider the rotation of the object. So area of the bounding rectangle won't be minimum. It is found by the function cv.boundingRect().\n",
    "\n",
    "**Syntax**: (x,y,w,h) = cv.boundingRect(array)\n",
    "\n",
    "* array: An array of 2-D points which form the object.\n",
    "* The function returns a tuple where:\n",
    " - (x,y) is the top-left coordinate of the rectangle\n",
    " - (w,h) is its width and height. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w,h = cv2.boundingRect(cnt)\n",
    "cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Rotated Rectangle ( Minimum Area Recrtangle)\n",
    "\n",
    "Here, bounding rectangle is drawn with minimum area, so it considers the rotation also. The function used is cv.minAreaRect(). \n",
    "\n",
    "**Syntax**: (center(x,y), (width, height), angle of rotation) = cv.minAreaRect(array)\n",
    "\n",
    "* array: An array of 2-D points which form the object.\n",
    "* The function returns a tuple as shown above.\n",
    "* To draw the rectangle we'll need the coordinates of the 4 corner points of the rectangle.\n",
    " - The  cv.boxPoints() function is used for this. \n",
    " - **Syntax**: points = cv.boxPoints(box)\n",
    " - points: The output array of four vertices of rectangles. \n",
    " - box: The input rotated rectangle.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rect = cv2.minAreaRect(cnt)\n",
    "print(rect)\n",
    "box = cv2.boxPoints(rect)\n",
    "box = np.int0(box)\n",
    "print(box)\n",
    "cv2.drawContours(img1,[box],0,(0,0,255),2)\n",
    "cv2.imshow('contours', img1)\n",
    "if cv2.waitKey(0) & 0xff == 27:  \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textile Quality Analysis:\n",
    "**We determine the metrics that determine the quality of fabrics**\n",
    "\n",
    "**One of the metrics that determine the quality of a fabric is warp & weft count** \n",
    "\n",
    "![IMG-20190414-WA0013.jpeg](./nb_images/tqa1.png)\n",
    "\n",
    "![IMG-20190414-WA0017.jpeg](./nb_images/tqa2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE  \n",
    "**Now, you know how to find the area of an contour. The task is to now find the contour of the maximum area and draw it.**\n",
    "\n",
    "**You have 5 minutes.**\n",
    "\n",
    "**Hint: Loop through contours.**\n",
    "\n",
    "![cvi4.jpg](./test_imgs/cvi4.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: Find the contour of largest area in './test_imgs/cvi4.jpg'(which appears as the image above) image\n",
    "#and draw a contour around it.\n",
    "\n",
    "#Steps to follow:\n",
    "    \n",
    "# Use the functions you had learnt in the previous session\n",
    "\n",
    "\n",
    "# Read image with label - './test_imgs/cvi4.jpg'\n",
    "\n",
    "# Create grayscale copies of the images\n",
    "\n",
    "#Threshold the grayscale image and find all contours in this image\n",
    "\n",
    "#Loop through all contours in the list of all contours and store the index of largest contour\n",
    "\n",
    "#Draw the largest contour and display the image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
